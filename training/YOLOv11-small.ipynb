{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee538be7-4035-4216-b6e0-26d03b9bc5d3",
   "metadata": {},
   "source": [
    "# YOLOv11-Small Model Image Classification\n",
    "Based on Edje Electronic's YOLO Tutorial:\n",
    "###### https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b068f4-a61b-46ca-a99d-85e9bc31147e",
   "metadata": {},
   "source": [
    "### Split the dataset\n",
    "Use the functions defined in `../data/data_splitter.py` to randomly split the given dataset into 80% for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0505c491-dba8-4021-b28d-e8516adb201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image files: 162\n",
      "Number of annotation files: 162\n",
      "Images moving to train: 129\n",
      "Images moving to test: 33\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "import data.data_splitter as ds\n",
    "path = '../data/candy_data_06JAN25'\n",
    "ds.split_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ea611-27d0-41a9-bf49-27939993b30a",
   "metadata": {},
   "source": [
    "### Configure Training\n",
    "Use the function defined in `../data/data_yaml_generator.py` to create the YAML data file for training and the `yolo detect train` function given by Ultralytics for training the YOLOv11 model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d321725-715a-4aac-bc5b-411dd9b43f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Updated script is running\n",
      "classes: ['MMs_peanut', 'MMs_regular', 'airheads', 'gummy_worms', 'milky_way', 'nerds', 'skittles', 'snickers', 'starburst', 'three_musketeers', 'twizzlers']\n",
      "train_path: candy_data_06JAN25-split/train/images\n",
      "val_path: candy_data_06JAN25-split/test/images\n",
      "train_path: candy_data_06JAN25-split/train/images, Created config file at ../data/data.yaml\n",
      "Ultralytics 8.3.126 ðŸš€ Python-3.12.9 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090, 24077MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../data/data.yaml, degrees=0.0, deterministic=True, device=cuda:0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=60, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train16, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train16, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    823665  ultralytics.nn.modules.head.Detect           [11, [128, 256, 512]]         \n",
      "YOLO11s summary: 181 layers, 9,432,049 parameters, 9,432,033 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5280.4Â±1420.4 MB/s, size: 224.3 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/motionprediction/.vscode/extensions/COE379L-robotic-arm-nl\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/motionprediction/.vscode/extensions/COE379L-robotic-arm-nlp/data/candy_data_06JAN25-split/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3679.3Â±2157.1 MB/s, size: 294.7 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/motionprediction/.vscode/extensions/COE379L-robotic-arm-nlp/\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/motionprediction/.vscode/extensions/COE379L-robotic-arm-nlp/data/candy_data_06JAN25-split/test/labels.cache\n",
      "Plotting labels to runs/detect/train16/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000667, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train16\u001b[0m\n",
      "Starting training for 60 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/60      4.04G     0.8438      4.257      1.063          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.289      0.152     0.0888     0.0789\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/60      4.24G     0.6912       2.88     0.9748          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139        0.3      0.464      0.328       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/60      4.26G     0.6542      1.992     0.9619          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.377      0.679      0.568      0.505\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/60      4.26G     0.6203      1.521      0.915         10        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.713      0.703      0.761      0.673\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/60      4.26G     0.6168      1.247     0.9312          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.597       0.82      0.765      0.677\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/60      4.26G     0.6602      1.089     0.9398          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.765      0.855       0.87      0.781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/60      4.26G     0.6381      1.016     0.9486          6        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.711      0.852      0.831      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/60      4.27G     0.6451     0.9581     0.9449         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.779      0.845      0.897      0.782\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/60      4.27G     0.6965      0.887      0.978          5        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.887      0.847      0.933      0.815\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/60      4.27G     0.5901     0.7429     0.9273          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.843      0.862      0.933       0.82\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/60      4.27G     0.6168     0.7269     0.9229          5        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.878      0.853      0.933      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/60      4.27G        0.6     0.6486     0.9145          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.905      0.902      0.967      0.835\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/60      4.27G     0.5804     0.6551      0.898          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.881      0.863      0.957      0.838\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/60      4.27G     0.5724     0.6346     0.9241          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139       0.88       0.86      0.954       0.83\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/60      4.27G     0.5687     0.6309     0.9153          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.916      0.912       0.96      0.847\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/60      4.27G     0.5528      0.618      0.904          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.904      0.881      0.959      0.851\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/60      4.27G      0.503      1.232     0.8088          0        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139       0.96      0.949      0.978      0.867\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/60      4.27G     0.7401      0.672      1.232          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.955      0.949      0.976      0.872\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/60      4.27G     0.5502     0.5638     0.9106          6        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.926      0.924      0.974      0.868\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/60      4.27G      0.659     0.5766     0.9749          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.949      0.938      0.974      0.866\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/60      4.27G     0.5275     0.4616     0.9172          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.933      0.967       0.97      0.869\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/60      4.27G     0.5318     0.4818      0.893          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139       0.96      0.962      0.971      0.875\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/60      4.27G     0.5445     0.4696     0.8965          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.926      0.941      0.957      0.861\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/60      4.27G     0.5364     0.4814     0.8904         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.923      0.944      0.938      0.839\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/60      4.27G       0.51     0.4662     0.8804         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.949      0.929      0.952      0.861\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/60      4.27G     0.5516     0.4727     0.9175         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.935      0.911      0.944      0.845\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/60      4.27G     0.5693     0.4947     0.8593          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.915      0.942      0.951      0.853\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/60      4.27G     0.5065     0.4283     0.8915         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.949      0.951      0.975       0.88\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/60      4.27G     0.5167     0.4269     0.9082          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.948      0.955      0.977      0.873\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/60      4.27G     0.5163     0.4024     0.9044          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.947      0.956      0.978      0.882\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/60      4.27G     0.5141     0.4918     0.8974          5        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.956       0.95      0.976      0.875\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/60      4.27G     0.5848     0.4989     0.9106          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.956       0.96      0.977      0.879\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/60      4.27G     0.5129     0.4215     0.8841          9        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.943      0.968      0.973      0.877\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/60      4.27G     0.4929     0.3937     0.8971          6        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.971      0.966      0.975      0.879\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/60      4.27G      0.486     0.4017     0.8824          3        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.952       0.97      0.971      0.879\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/60      4.27G     0.5036     0.3946     0.8897          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.937      0.975      0.973      0.881\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/60      4.27G     0.4789     0.4098      0.881          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.943      0.972      0.975      0.886\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/60      4.27G     0.4691     0.3716     0.8832          3        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.943      0.971      0.974      0.885\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/60      4.27G     0.5095     0.4018     0.9336          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.957       0.97      0.974       0.88\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/60      4.27G     0.4648     0.3574     0.8706          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.955       0.97      0.973      0.892\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/60      4.27G     0.5267     0.3777     0.9013          3        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.957      0.963      0.973      0.889\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/60      4.27G     0.4637     0.3523     0.8706          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.952      0.961      0.972      0.891\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/60      4.27G     0.4972     0.4404     0.9037          3        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.956      0.965      0.969      0.889\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/60      4.27G     0.4788     0.4273     0.8949          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139       0.96      0.957      0.967      0.881\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/60      4.27G     0.4814     0.3632     0.8571         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.949      0.965      0.969      0.881\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/60      4.27G     0.4549     0.3493     0.8812          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.945      0.975       0.97      0.883\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/60      4.27G     0.4987     0.3822      0.922          3        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.955      0.969      0.971      0.883\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/60      4.27G     0.4617     0.3951      0.892          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.964       0.96      0.971      0.884\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/60      4.27G     0.4319     0.3395     0.8671         11        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.977      0.949      0.971      0.886\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/60      4.27G     0.4296     0.3187     0.8738         11        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.959      0.953      0.974      0.891\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      51/60      4.27G     0.4389     0.4999     0.8286          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.956      0.952       0.97       0.89\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      52/60      4.27G     0.3633     0.2877     0.8099          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.945      0.947       0.97      0.882\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      53/60      4.27G     0.4403      0.399     0.8194          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.943      0.947      0.969      0.886\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      54/60      4.27G     0.3385     0.2768     0.7937          6        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139       0.94      0.944      0.968      0.885\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      55/60      4.27G     0.3473     0.2534     0.8109          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.939      0.941      0.967      0.884\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      56/60      4.27G     0.3429     0.2591     0.7896          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.936      0.927      0.968      0.889\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      57/60      4.27G     0.3365     0.2523     0.8046          6        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.935      0.933      0.969      0.891\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      58/60      4.27G     0.4109     0.2954     0.8403          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.933      0.942      0.969      0.892\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      59/60      4.27G     0.3397     0.2525     0.8258          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.934      0.942      0.969       0.89\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      60/60      4.27G     0.3583     0.3416     0.8327          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.947       0.93      0.969      0.888\n",
      "\n",
      "60 epochs completed in 0.011 hours.\n",
      "Optimizer stripped from runs/detect/train16/weights/last.pt, 19.2MB\n",
      "Optimizer stripped from runs/detect/train16/weights/best.pt, 19.2MB\n",
      "\n",
      "Validating runs/detect/train16/weights/best.pt...\n",
      "Ultralytics 8.3.126 ðŸš€ Python-3.12.9 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4090, 24077MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,417,057 parameters, 0 gradients, 21.3 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         33        139      0.955       0.97      0.973      0.892\n",
      "            MMs_peanut         11         11      0.962          1      0.995      0.937\n",
      "           MMs_regular         12         13      0.908          1      0.995      0.912\n",
      "              airheads         17         24      0.922          1      0.987      0.945\n",
      "           gummy_worms         14         14      0.978          1      0.995      0.927\n",
      "             milky_way         10         10          1      0.888      0.915      0.804\n",
      "                 nerds         10         11          1       0.93      0.995      0.902\n",
      "              skittles         14         16      0.933          1      0.995      0.921\n",
      "              snickers          7          7      0.839          1      0.906      0.786\n",
      "             starburst          9         10      0.978          1      0.995      0.928\n",
      "      three_musketeers         13         13      0.986      0.923      0.934      0.848\n",
      "             twizzlers         10         10          1      0.933      0.995      0.906\n",
      "Speed: 0.1ms preprocess, 0.6ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train16\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
     ]
    }
   ],
   "source": [
    "# Create a YAML configuration file for YOLO\n",
    "import data.data_yaml_generator as dyg\n",
    "yaml_file = '../data/data.yaml'\n",
    "classes_file = f'{path}-split/classes.txt'\n",
    "dyg.create_data_yaml(classes_file, yaml_file)\n",
    "\n",
    "# Train the model using the YAML configuration file and YOLO model\n",
    "RETRAIN_MODEL = True\n",
    "if RETRAIN_MODEL or not os.path.exists('yolo11s.pt'):\n",
    "    !yolo detect train data=../data/data.yaml model=yolo11s.pt epochs=60 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f78c4d1-22dc-4b4c-978b-af6107f41a97",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e039c3-8466-4a0e-bed6-2f9721ed6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/jacob/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57902f60-f038-45fb-8b13-d173acc7546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './runs/detect/train/weights/best.pt'\n",
    "img_source = '../data/candy-test/testvid.mp4'\n",
    "# min_thresh = \n",
    "user_res = '1280x720'\n",
    "record = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46d0a2e-748d-413d-92c1-5c93cc196f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found\n"
     ]
    }
   ],
   "source": [
    "if (not os.path.exists(model_path)):\n",
    "    print('ERROR: Model path is invalid or model was not found. Make sure the model filename was entered correctly.')\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print('Model found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce18a08d-1b86-4311-bea9-82797278666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(model_path, task='detect')\n",
    "labels = model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c62c31-b30a-4fa7-bd8a-27a530dcb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse input to determine if image source is a file, folder, video, or USB camera\n",
    "img_ext_list = ['.jpg','.JPG','.jpeg','.JPEG','.png','.PNG','.bmp','.BMP']\n",
    "vid_ext_list = ['.avi','.mov','.mp4','.mkv','.wmv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8c9fc0-312d-4e11-86ba-b9b2a2768f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(img_source):\n",
    "    source_type = 'folder'\n",
    "elif os.path.isfile(img_source):\n",
    "    _, ext = os.path.splitext(img_source)\n",
    "    if ext in img_ext_list:\n",
    "        source_type = 'image'\n",
    "    elif ext in vid_ext_list:\n",
    "        source_type = 'video'\n",
    "    else:\n",
    "        print(f'File extension {ext} is not supported.')\n",
    "        sys.exit(0)\n",
    "elif 'usb' in img_source:\n",
    "    source_type = 'usb'\n",
    "    usb_idx = int(img_source[3:])\n",
    "elif 'picamera' in img_source:\n",
    "    source_type = 'picamera'\n",
    "    picam_idx = int(img_source[8:])\n",
    "else:\n",
    "    print(f'Input {img_source} is invalid. Please try again.')\n",
    "    sys.exit(0)\n",
    "\n",
    "print(source_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f1e69f5-1590-4254-8dfa-cdbb98682fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 720\n"
     ]
    }
   ],
   "source": [
    "# Parse user-specified display resolution\n",
    "resize = False\n",
    "if user_res:\n",
    "    resize = True\n",
    "    resW, resH = int(user_res.split('x')[0]), int(user_res.split('x')[1])\n",
    "\n",
    "print(resW, resH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8217a29d-c75c-46c5-a368-045385d698fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if recording is valid and set up recording\n",
    "if record:\n",
    "    if source_type not in ['video','usb']:\n",
    "        print('Recording only works for video and camera sources. Please try again.')\n",
    "        sys.exit(0)\n",
    "    if not user_res:\n",
    "        print('Please specify resolution to record video at.')\n",
    "        sys.exit(0)\n",
    "    \n",
    "    # Set up recording\n",
    "    record_name = 'demo1.avi'\n",
    "    record_fps = 30\n",
    "    recorder = cv2.VideoWriter(record_name, cv2.VideoWriter_fourcc(*'MJPG'), record_fps, (resW,resH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ff03a-296b-4f33-a2ea-6d8a3a64c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or initialize image source\n",
    "if source_type == 'image':\n",
    "    imgs_list = [img_source]\n",
    "elif source_type == 'folder':\n",
    "    imgs_list = []\n",
    "    filelist = glob.glob(img_source + '/*')\n",
    "    for file in filelist:\n",
    "        _, file_ext = os.path.splitext(file)\n",
    "        if file_ext in img_ext_list:\n",
    "            imgs_list.append(file)\n",
    "elif source_type == 'video' or source_type == 'usb':\n",
    "    if source_type == 'video': cap_arg = img_source\n",
    "    elif source_type == 'usb': cap_arg = usb_idx\n",
    "    cap = cv2.VideoCapture(cap_arg)\n",
    "\n",
    "    # Set camera or video resolution if specified by user\n",
    "    if user_res:\n",
    "        ret = cap.set(3, resW)\n",
    "        ret = cap.set(4, resH)\n",
    "elif source_type == 'picamera':\n",
    "    from picamera2 import Picamera2\n",
    "    cap = Picamera2()\n",
    "    cap.configure(cap.create_video_configuration(main={\"format\": 'RGB888', \"size\": (resW, resH)}))\n",
    "    cap.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61003b78-6055-488e-9675-af66a4db5544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bounding box colors (using the Tableu 10 color scheme)\n",
    "bbox_colors = [(164,120,87), (68,148,228), (93,97,209), (178,182,133), (88,159,106), \n",
    "              (96,202,231), (159,124,168), (169,162,241), (98,118,150), (172,176,184)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d45f85a-5b50-45a0-9115-d65064f3bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize control and status variables\n",
    "avg_frame_rate = 0\n",
    "frame_rate_buffer = []\n",
    "fps_avg_len = 200\n",
    "img_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "485cde01-eb78-466d-ae31-3a70d9f7043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached end of the video file. Exiting program.\n"
     ]
    }
   ],
   "source": [
    "# Define the video codec and create a VideoWriter object\n",
    "output_path = 'output_video.avi'  # Set your desired output path and filename\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Choose codec (e.g., XVID, MJPG)\n",
    "fps = 20  # Set frames per second (adjust to your needs)\n",
    "\n",
    "# Create VideoWriter object\n",
    "recorder = cv2.VideoWriter(output_path, fourcc, fps, (resW, resH))\n",
    "\n",
    "# Begin inference loop\n",
    "while True:\n",
    "\n",
    "    t_start = time.perf_counter()\n",
    "    # Load frame from image source\n",
    "    if source_type == 'image' or source_type == 'folder': # If source is image or image folder, load the image using its filename\n",
    "        if img_count >= len(imgs_list):\n",
    "            print('All images have been processed. Exiting program.')\n",
    "            sys.exit(0)\n",
    "        img_filename = imgs_list[img_count]\n",
    "        frame = cv2.imread(img_filename)\n",
    "        img_count = img_count + 1\n",
    "    \n",
    "    elif source_type == 'video': # If source is a video, load next frame from video file\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print('Reached end of the video file. Exiting program.')\n",
    "            break\n",
    "    \n",
    "    elif source_type == 'usb': # If source is a USB camera, grab frame from camera\n",
    "        ret, frame = cap.read()\n",
    "        if (frame is None) or (not ret):\n",
    "            print('Unable to read frames from the camera. This indicates the camera is disconnected or not working. Exiting program.')\n",
    "            break\n",
    "\n",
    "    elif source_type == 'picamera': # If source is a Picamera, grab frames using picamera interface\n",
    "        frame = cap.capture_array()\n",
    "        if (frame is None):\n",
    "            print('Unable to read frames from the Picamera. This indicates the camera is disconnected or not working. Exiting program.')\n",
    "            break\n",
    "\n",
    "    # Resize frame to desired display resolution\n",
    "    if resize == True:\n",
    "        frame = cv2.resize(frame,(resW,resH))\n",
    "\n",
    "    # Run inference on frame\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    # Extract results\n",
    "    detections = results[0].boxes\n",
    "\n",
    "    # Initialize variable for basic object counting example\n",
    "    object_count = 0\n",
    "\n",
    "    # Go through each detection and get bbox coords, confidence, and class\n",
    "    for i in range(len(detections)):\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        # Ultralytics returns results in Tensor format, which have to be converted to a regular Python array\n",
    "        xyxy_tensor = detections[i].xyxy.cpu() # Detections in Tensor format in CPU memory\n",
    "        xyxy = xyxy_tensor.numpy().squeeze() # Convert tensors to Numpy array\n",
    "        xmin, ymin, xmax, ymax = xyxy.astype(int) # Extract individual coordinates and convert to int\n",
    "\n",
    "        # Get bounding box class ID and name\n",
    "        classidx = int(detections[i].cls.item())\n",
    "        classname = labels[classidx]\n",
    "\n",
    "        # Get bounding box confidence\n",
    "        conf = detections[i].conf.item()\n",
    "\n",
    "        # Draw box if confidence threshold is high enough\n",
    "        if conf > 0.5:\n",
    "\n",
    "            color = bbox_colors[classidx % 10]\n",
    "            cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), color, 2)\n",
    "\n",
    "            label = f'{classname}: {int(conf*100)}%'\n",
    "            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1) # Get font size\n",
    "            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "            cv2.rectangle(frame, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), color, cv2.FILLED) # Draw white box to put label text in\n",
    "            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1) # Draw label text\n",
    "\n",
    "            # Basic example: count the number of objects in the image\n",
    "            object_count = object_count + 1\n",
    "\n",
    "    # Calculate and draw framerate (if using video, USB, or Picamera source)\n",
    "    if source_type == 'video' or source_type == 'usb' or source_type == 'picamera':\n",
    "        cv2.putText(frame, f'FPS: {avg_frame_rate:0.2f}', (10,20), cv2.FONT_HERSHEY_SIMPLEX, .7, (0,255,255), 2) # Draw framerate\n",
    "    \n",
    "    # # Display detection results\n",
    "    # cv2.putText(frame, f'Number of objects: {object_count}', (10,40), cv2.FONT_HERSHEY_SIMPLEX, .7, (0,255,255), 2) # Draw total number of detected objects\n",
    "    # cv2.imshow('YOLO detection results',frame) # Display image\n",
    "    # if record: recorder.write(frame)\n",
    "\n",
    "    # If inferencing on individual images, wait for user keypress before moving to next image. Otherwise, wait 5ms before moving to next frame.\n",
    "    if source_type == 'image' or source_type == 'folder':\n",
    "        key = cv2.waitKey()\n",
    "    elif source_type == 'video' or source_type == 'usb' or source_type == 'picamera':\n",
    "        key = cv2.waitKey(5)\n",
    "    \n",
    "    if key == ord('q') or key == ord('Q'): # Press 'q' to quit\n",
    "        break\n",
    "    elif key == ord('s') or key == ord('S'): # Press 's' to pause inference\n",
    "        cv2.waitKey()\n",
    "    elif key == ord('p') or key == ord('P'): # Press 'p' to save a picture of results on this frame\n",
    "        cv2.imwrite('capture.png',frame)\n",
    "    \n",
    "    # Calculate FPS for this frame\n",
    "    t_stop = time.perf_counter()\n",
    "    frame_rate_calc = float(1/(t_stop - t_start))\n",
    "\n",
    "    # Append FPS result to frame_rate_buffer (for finding average FPS over multiple frames)\n",
    "    if len(frame_rate_buffer) >= fps_avg_len:\n",
    "        temp = frame_rate_buffer.pop(0)\n",
    "        frame_rate_buffer.append(frame_rate_calc)\n",
    "    else:\n",
    "        frame_rate_buffer.append(frame_rate_calc)\n",
    "\n",
    "    # Calculate average FPS for past frames\n",
    "    avg_frame_rate = np.mean(frame_rate_buffer)\n",
    "\n",
    "    if record:\n",
    "        recorder.write(frame)\n",
    "\n",
    "recorder.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e5fa22a-f541-4a57-8652-83ee0748d5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average pipeline FPS: 9.44\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "print(f'Average pipeline FPS: {avg_frame_rate:.2f}')\n",
    "if source_type == 'video' or source_type == 'usb':\n",
    "    cap.release()\n",
    "elif source_type == 'picamera':\n",
    "    cap.stop()\n",
    "if record: recorder.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
